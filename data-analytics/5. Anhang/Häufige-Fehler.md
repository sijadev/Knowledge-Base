---
autor: Simon Janke
title: H√§ufige Datenanalyse-Fehler
type: Ressource
date: 2025-10-12
tags:
  - fehler
  - pitfalls
  - antipatterns
  - lessons-learned
---

# ‚ö†Ô∏è H√§ufige Datenanalyse-Fehler & Wie man sie vermeidet

Ein umfassender Guide zu den klassischen Fallstricken in der Datenanalyse.

---

## üî¥ Kategorie 1: Statistische Fehler

### Fehler 1.1: Korrelation als Kausalit√§t interpretieren

**‚ùå Der Fehler:**
```
Beobachtung: Eisverk√§ufe und Ertrinkungsunf√§lle korrelieren positiv

Falsche Schlussfolgerung:
"Eisverk√§ufe verursachen Ertrinken ‚Üí Eisverkauf verbieten!"

Richtig:
Beide werden durch einen dritten Faktor (Sommer/Hitze) verursacht
```

**Reales Business-Beispiel:**
```
Daten zeigen: 
- Kunden mit Premium-Support haben 50% h√∂here Retention
- Manch manager: "Geben wir allen Premium-Support!"

ABER: Causation ‚â† Correlation
- Premium-Kunden sind bereits high-value und engaged
- Support ist Korrelat, nicht Ursache der Retention
- Experiment n√∂tig um Kausalit√§t zu beweisen
```

**‚úÖ So vermeiden:**
1. Immer fragen: "K√∂nnte X auch Y verursachen?" (reverse causation)
2. "Gibt es einen dritten Faktor Z?"
3. Nur A/B-Tests oder kontrollierte Experimente beweisen Kausalit√§t
4. Bei starker Korrelation: Hypothese bilden, dann testen

---

### Fehler 1.2: P-Hacking / Cherry Picking

**‚ùå Der Fehler:**
```
Analyst testet 20 verschiedene Hypothesen und berichtet nur die eine,
die p < 0.05 zeigt, ohne zu erw√§hnen, dass 19 andere tests durchgef√ºhrt wurden.

Bei 20 Tests mit Œ±=0.05 erwarte ich 1 "false positive" by chance!
```

**Beispiel:**
```
Test 1: Mobile vs Desktop Conversion ‚Üí p = 0.32 ‚ùå
Test 2: iOS vs Android Conversion ‚Üí p = 0.18 ‚ùå
Test 3: Chrome vs Safari Conversion ‚Üí p = 0.71 ‚ùå
...
Test 15: Dienstag vs Donnerstag Conversion ‚Üí p = 0.03 ‚úì

Report: "Dienstag hat signifikant h√∂here Conversion!"
(ohne die 14 failed tests zu erw√§hnen)
```

**‚úÖ So vermeiden:**
1. **Pre-register** Hypothesen vor Analyse
2. **Bonferroni-Korrektur** bei multiple testing (p-Wert / Anzahl Tests)
3. Alle Tests dokumentieren, nicht nur signifikante
4. Replikation in neuen Daten

---

### Fehler 1.3: Simpson's Paradox ignorieren

**‚ùå Der Fehler:**
Ein Trend in aggregierten Daten kehrt sich um, wenn man nach Gruppen segmentiert.

**Klassisches Beispiel - Uni-Admission:**
```
Gesamt:
- 40% M√§nner zugelassen
- 30% Frauen zugelassen
‚Üí "Bias gegen Frauen!"

Pro Department:
Dept A: 60% M√§nner, 70% Frauen zugelassen
Dept B: 20% M√§nner, 30% Frauen zugelassen

Problem: Frauen bewerben sich mehr bei Dept B (schwerer)
```

**Business-Beispiel:**
```
Gesamt: Kampagne A hat h√∂here Conversion als B

Aber segmentiert:
- Mobile: B besser als A
- Desktop: B besser als A

Erkl√§rung: A bekommt mehr Desktop-Traffic (konvertiert generell besser)
```

**‚úÖ So vermeiden:**
1. **Immer segmentieren** nach wichtigen Dimensionen
2. Confounding Variables identifizieren
3. Multivariate Analysis statt univariate

---

### Fehler 1.4: Zu kleine Stichproben

**‚ùå Der Fehler:**
```
A/B-Test mit 50 Nutzern pro Variante
‚Üí 40% vs 30% Conversion
‚Üí "Variante A gewinnt!"

ABER: Mit so wenig Daten ist das pure Zufallsschwankung
```

**Minimum Sample Sizes:**
```
F√ºr aussagekr√§ftige A/B-Tests:
- Conversion ~5%: >2000 pro Variante
- Conversion ~10%: >1000 pro Variante  
- Conversion ~25%: >400 pro Variante

Minimum f√ºr jede Analyse: >30 (Central Limit Theorem)
Besser: >100
Ideal: >1000
```

**‚úÖ So vermeiden:**
1. **Sample Size Calculator** vor Test nutzen
2. Power Analysis durchf√ºhren
3. Mindestlaufzeit einhalten (1-2 Wochen f√ºr A/B-Tests)
4. Bei kleinen Samples: Vorsicht mit Schlussfolgerungen

---

### Fehler 1.5: Survivorship Bias

**‚ùå Der Fehler:**
Nur "√úberlebende" analysieren und daraus Schl√ºsse ziehen.

**Klassisches Beispiel - WW2 Flugzeuge:**
```
Milit√§r analysiert zur√ºckgekehrte Flugzeuge
‚Üí Besch√§digungen an Fl√ºgeln und Rumpf
‚Üí Vorschlag: Diese Bereiche verst√§rken

FEHLER: Die Flugzeuge mit Motor-Sch√§den sind NICHT zur√ºckgekehrt!
‚Üí Motoren m√ºssen verst√§rkt werden
```

**Business-Beispiele:**
```
1. "Erfolgreiche Startups haben alle fr√ºh gepivoted"
   ‚Üí Falsch: Du siehst nicht die, die gepivoted UND gescheitert sind
   
2. "Unsere besten Kunden nutzen alle Feature X"
   ‚Üí Was ist mit Churn? Haben die auch Feature X genutzt?
   
3. "Nutzer die > 10 Sessions haben, werden loyal"
   ‚Üí Reverse: Loyale Nutzer haben > 10 Sessions (nicht Kausalit√§t)
```

**‚úÖ So vermeiden:**
1. **Kontrolle f√ºr "Nicht-√úberlebende"** (Churn, Failed Projects, etc.)
2. **Cohort-Analysen** (alle Nutzer tracken, nicht nur aktive)
3. Frage: "Wen sehe ich NICHT in den Daten?"

---

## üü° Kategorie 2: Datenqualit√§ts-Fehler

### Fehler 2.1: Garbage In, Garbage Out

**‚ùå Der Fehler:**
Analyse auf schmutzigen Daten ohne Validierung.

**Beispiele:**
```
1. Tracking-Fehler:
   - Event fired 3x statt 1x
   - ‚Üí Revenue 3x √ºbersch√§tzt
   
2. Duplikate:
   - Gleicher User mit 5 IDs
   - ‚Üí Unique Users 5x √ºbersch√§tzt
   
3. Fehlende Werte als 0:
   - NULL Revenue ‚Üí 0
   - ‚Üí Durchschnitt falsch berechnet
   
4. Typ-Fehler:
   - "123" (string) + "456" = "123456" statt 579
```

**‚úÖ So vermeiden:**
1. **Data Quality Checks** IMMER first step:
   ```sql
   -- Duplikate check
   SELECT id, COUNT(*) 
   FROM table GROUP BY id HAVING COUNT(*) > 1;
   
   -- NULL check
   SELECT COUNT(*) AS nulls 
   FROM table WHERE important_field IS NULL;
   
   -- Outliers check
   SELECT MIN(value), MAX(value), AVG(value), STDDEV(value)
   FROM table;
   ```

2. **Profiling** vor Analyse
3. **Stichproben manuell pr√ºfen**
4. **Validierung gegen bekannte Benchmarks**

---

### Fehler 2.2: Sampling Bias

**‚ùå Der Fehler:**
Stichprobe ist nicht repr√§sentativ f√ºr Population.

**Beispiele:**
```
1. Online-Survey Bias:
   - Nur tech-savvy Nutzer antworten
   - ‚Üí √úbersch√§tzt digitale Affinit√§t
   
2. Voluntary Response Bias:
   - Nur sehr zufriedene/unzufriedene antworten
   - ‚Üí Extreme Meinungen √ºberrepr√§sentiert
   
3. Time-of-Day Bias:
   - Analyse nur Workday-Daten
   - ‚Üí √úbersieht Weekend-Nutzer
   
4. Geographic Bias:
   - Test nur in US
   - ‚Üí Nicht √ºbertragbar auf Europa/Asia
```

**‚úÖ So vermeiden:**
1. **Random Sampling** wo m√∂glich
2. **Stratified Sampling** f√ºr wichtige Segmente
3. **Quotas** f√ºr Ausgewogenheit
4. **Weights** f√ºr Repr√§sentativit√§t
5. Dokumentiere Sampling-Methode

---

### Fehler 2.3: Daten-Leakage

**‚ùå Der Fehler:**
Future Information in Training-Daten (bei ML-Modellen).

**Beispiel:**
```
Problem: Churn Prediction
Features: 
- Customer_ID
- Last_Login_Date  ‚Üê LEAKAGE!
- Support_Tickets
- Churned (Target)

Warum Leakage?
- Gechurnete Kunden haben nat√ºrlich kein recent Login
- Modell lernt: "Kein Login = Churn"
- ABER: In Produktion wei√ü ich nicht wer einloggen wird!
```

**‚úÖ So vermeiden:**
1. **Temporal Validation**: Train auf Vergangenheit, test auf Zukunft
2. Nur Features nutzen, die **vor** Target bekannt sind
3. "K√∂nnte ich diese Info zum Predictions-Zeitpunkt haben?"

---

## üü† Kategorie 3: Analyse-Design-Fehler

### Fehler 3.1: HARKing (Hypothesizing After Results are Known)

**‚ùå Der Fehler:**
Nach dem Sehen der Daten eine "Hypothese" formulieren, als w√§re sie vor der Analyse erstellt worden.

**Beispiel:**
```
Analyst schaut Daten an:
"Oh, Mobile Conversion ist 10% niedriger!"

Report:
"Wir HYPOTHESIERTEN dass Mobile Conversion niedriger sei..."

Problem: Das ist keine echte Hypothese, sondern post-hoc Erkl√§rung
```

**‚úÖ So vermeiden:**
1. **Pre-register** Hypothesen
2. Exploratory vs. Confirmatory Analysis trennen
3. Transparent √ºber Discovery vs. Validation

---

### Fehler 3.2: One-Size-Fits-All Metrics

**‚ùå Der Fehler:**
Gleiche Metrik f√ºr verschiedene Kontexte nutzen.

**Beispiele:**
```
1. "Average Revenue per User" (ARPU)
   Problem: 
   - Bei skewed distributions irref√ºhrend
   - Ein Whale-Kunde kann Durchschnitt verzerren
   Besser: Median + P90

2. "Average Session Duration"
   Problem:
   - L√§nger ‚â† Besser
   - Support-Seiten: Lang = Problem findet keine L√∂sung
   - Content-Seiten: Lang = Engagement
   Besser: Kontext-spezifische Metriken

3. "Click-Through-Rate" (CTR)
   Problem:
   - Hohe CTR, aber keine Conversion
   - Clickbait performiert "gut"
   Besser: Multi-metric evaluation
```

**‚úÖ So vermeiden:**
1. **North Star Metric** definieren (kontextabh√§ngig)
2. **Counter-Metrics** f√ºr jede Primary-Metric
3. **Guardrail Metrics** f√ºr Qualit√§t
4. Segmentierte Metriken

---

### Fehler 3.3: Comparing Apples to Oranges

**‚ùå Der Fehler:**
Unfairer Vergleich durch unterschiedliche Bedingungen.

**Beispiele:**
```
1. Zeitvergleich ohne Seasonality:
   "Dezember-Sales sind 200% h√∂her als November!"
   ‚Üí Nat√ºrlich wegen Weihnachten
   
2. Geo-Vergleich ohne Normalisierung:
   "USA hat 10x mehr Sales als Schweiz!"
   ‚Üí Ja, aber 40x mehr Einwohner
   
3. Produkt-Vergleich mit unterschiedlichem Age:
   "Product A hat mehr Users als B"
   ‚Üí A ist seit 5 Jahren live, B seit 2 Monaten
```

**‚úÖ So vermeiden:**
1. **Like-for-like** Vergleiche
2. **Normalisierung** (per Capita, per Day, etc.)
3. **Cohort-Vergleiche** statt absolute
4. **Control f√ºr Confounders**

---

## üîµ Kategorie 4: Kommunikations-Fehler

### Fehler 4.1: Fehlender Kontext

**‚ùå Der Fehler:**
Zahlen ohne Vergleichswerte pr√§sentieren.

**Schlecht:**
```
"Wir haben 10.000 neue User!"
```

**Besser:**
```
"Wir haben 10.000 neue User:
- vs 8.000 letzten Monat (+25%)
- vs Target von 12.000 (-17%)
- vs Industry Growth von +15% (outperforming)
- Haupttreiber: Organic Search (+40%)
```

**‚úÖ So vermeiden:**
Immer 3 Kontexte geben:
1. **Temporal**: vs Vorperiode
2. **Target**: vs Ziel/Benchmark
3. **Why**: Treiber der Ver√§nderung

---

### Fehler 4.2: √úberkomplizierte Visualisierungen

**‚ùå Der Fehler:**
```
- 15 Lines in einem Chart
- 3D Pie Charts
- Dual Axes mit unterschiedlichen Skalen
- Zu viele Farben
```

**‚úÖ So vermeiden:**
1. **One Chart, One Message**
2. **Max 3-5 Series** in einem Chart
3. **Simple > Complex**
4. **Test**: "Kann jemand in 5 Sekunden verstehen?"

---

### Fehler 4.3: False Precision

**‚ùå Der Fehler:**
Zu viele Dezimalstellen suggerieren h√∂here Genauigkeit als existiert.

**Schlecht:**
```
"Conversion Rate ist 2.847291%"
```

**Besser:**
```
"Conversion Rate ist ~2.8%"
oder
"Conversion Rate ist 2.8% ¬± 0.3% (95% CI)"
```

**‚úÖ So vermeiden:**
1. Runde auf **significant figures**
2. **Konfidenzintervalle** statt Punktsch√§tzungen
3. "Approximately" / "About" / "~" nutzen

---

## üü¢ Kategorie 5: Business-Logic-Fehler

### Fehler 5.1: Optimierung der falschen Metrik

**‚ùå Der Fehler:**
Lokales Optimum statt globales.

**Beispiele:**
```
1. E-Commerce optimiert "Add to Cart" Rate
   ‚Üí Nutzer adden, aber kaufen nicht
   ‚Üí Revenue sinkt trotz h√∂herer Cart-Rate
   
2. Content-Site optimiert "Time on Page"
   ‚Üí Nutzer finden Info nicht ‚Üí lesen l√§nger
   ‚Üí Schlechte UX, aber "gute" Metrik
   
3. Support optimiert "Ticket Close Rate"
   ‚Üí Tickets schnell geschlossen ohne L√∂sung
   ‚Üí Customer Satisfaction sinkt
```

**‚úÖ So vermeiden:**
1. **End-to-End** Metriken priorisieren (Revenue, CSAT, Retention)
2. **Counter-Metrics** immer monitoren
3. **A/B Test** ganze Customer Journey
4. Qualitative Feedback zus√§tzlich zu Quantitative

---

### Fehler 5.2: Short-term vs Long-term Trade-off ignorieren

**‚ùå Der Fehler:**
Nur kurzfristige Gewinne optimieren, langfristige Sch√§den ignorieren.

**Beispiele:**
```
1. Aggressive Email-Kampagnen:
   Short-term: +20% Revenue
   Long-term: +50% Unsubscribes, Brand Damage
   
2. Discount-Strategie:
   Short-term: Mehr Sales
   Long-term: Kunden warten auf Discounts, Margin erodiert
   
3. Clickbait-Headlines:
   Short-term: Higher CTR
   Long-term: Lower Trust, Bounce Rate steigt
```

**‚úÖ So vermeiden:**
1. **Leading vs Lagging** Indicators tracken
2. **Cohort-basierte** Long-term Metrics
3. **LTV-Modelle** statt nur Acquisition
4. **Brand Health** Metrics parallel

---

### Fehler 5.3: Ignoring Implementation Costs

**‚ùå Der Fehler:**
Empfehlungen ohne Kosten-Nutzen-Analyse.

**Beispiel:**
```
Analyst: "Wenn wir Ladezeit um 1s reduzieren, steigt Conversion um 2%!"

CEO: "Great! Wie?"

Analyst: "√Ñhm... komplettes Rewrite der Infrastruktur?"

‚Üí 2% Conversion-Lift = $100k Revenue
‚Üí Rewrite kostet $1M + 6 Monate
‚Üí ROI negativ!
```

**‚úÖ So vermeiden:**
1. **Impact vs Effort** Matrix
2. **Quick Wins** identifizieren
3. **Engineering-Input** vor Empfehlungen
4. **Phased Rollout** Optionen

---

## üõ†Ô∏è Best Practices: Fehler-Pr√§vention

### 1. Pre-Analysis Checklist
```
‚ñ° Hypothese klar formuliert
‚ñ° Sample Size berechnet
‚ñ° Confounders identifiziert
‚ñ° Data Quality gepr√ºft
‚ñ° Bias-Quellen ber√ºcksichtigt
‚ñ° Success Criteria definiert
```

### 2. During Analysis
```
‚ñ° Dokumentiere jeden Schritt
‚ñ° Stichproben manuell pr√ºfen
‚ñ° Segmentierte Analysen
‚ñ° Sanity Checks (stimmen Totals?)
‚ñ° Peer Review Code/Queries
```

### 3. Post-Analysis
```
‚ñ° Results validieren (separate Dataset)
‚ñ° Alternative Erkl√§rungen pr√ºfen
‚ñ° Limitations dokumentieren
‚ñ° Confidence Levels angeben
‚ñ° Stakeholder-Feedback einholen
```

### 4. Communication
```
‚ñ° Kontext immer geben
‚ñ° Unsicherheit transparent machen
‚ñ° Visualisierungen simpel halten
‚ñ° Actionable Recommendations
‚ñ° Follow-up Plan definieren
```

---

## üìö Lernressourcen

**B√ºcher:**
- "Naked Statistics" - Charles Wheelan
- "How to Lie with Statistics" - Darrell Huff
- "The Signal and the Noise" - Nate Silver

**Papers:**
- "Statistical Mistakes and How to Avoid Them" - Various

**Online:**
- [Calling Bullshit](https://callingbullshit.org/) - Course on Data Reasoning
- [Spurious Correlations](https://tylervigen.com/spurious-correlations) - Fun Examples

---

## ‚úÖ Quick Reference Card

**Top 10 Fehler vermeiden:**
1. ‚úÖ Korrelation ‚â† Kausalit√§t
2. ‚úÖ Sample Size matters
3. ‚úÖ Segmentiere immer
4. ‚úÖ Check Data Quality first
5. ‚úÖ Beware Survivorship Bias
6. ‚úÖ Pre-register Hypothesen
7. ‚úÖ Give Context
8. ‚úÖ Monitor Counter-Metrics
9. ‚úÖ Consider Long-term
10. ‚úÖ Document Assumptions

**Frage bei jeder Analyse:**
- "K√∂nnte das Zufall sein?"
- "Wen/Was sehe ich NICHT in den Daten?"
- "Gibt es alternative Erkl√§rungen?"
- "Welche Annahmen mache ich?"
- "Wie robust ist meine Schlussfolgerung?"

---

*Aktualisiert: Oktober 2025*