---
autor: Simon Janke
title: Statistische Untersuchung von Daten
type: Google Data Analytics Kurs
date: 2025-10-14
tags:
  - "#statistik"
  - "#bias"
links:
  - "[[Grundlagen]]"
---
---
## âœ… Warum IMMER statistische Untersuchungen?

### 1. **Du musst deine Daten kennen, bevor du sie analysierst**

Ohne statistische Untersuchung weiÃŸt du nicht:

- Ob deine Daten sinnvoll sind
- Ob Fehler oder AusreiÃŸer existieren
- Ob deine geplante Analyse Ã¼berhaupt funktionieren kann

### 2. **Bias-Erkennung** (dein aktuelles Thema!)

Statistische Checks zeigen dir:

- UnterreprÃ¤sentierte Gruppen
- Verzerrungen in der Verteilung
- Fehlende Werte (oft nicht zufÃ¤llig!)

### 3. **Vermeidung von Fehlinterpretationen**

Ohne Stats kÃ¶nntest du:

- Falsche SchlÃ¼sse ziehen
- Korrelation mit KausalitÃ¤t verwechseln
- Wichtige Muster Ã¼bersehen

## ğŸ“Š Standard-Prozess: Exploratory Data Analysis (EDA)

Das hast du sicher in deiner Google-Schulung gelernt! EDA ist der **erste Schritt** nach dem Daten-Import:

### **Phase 1: Ãœberblick verschaffen**

```
- Anzahl Zeilen/Spalten
- Datentypen prÃ¼fen
- Erste Zeilen anschauen (head/tail)
- Struktur verstehen
```

### **Phase 2: Deskriptive Statistik**

**FÃ¼r numerische Variablen:**

- Mean (Durchschnitt)
- Median (Mittelwert)
- Standardabweichung
- Min/Max
- Quartile (25%, 50%, 75%)

**FÃ¼r kategorische Variablen:**

- HÃ¤ufigkeitsverteilungen
- Anzahl eindeutiger Werte
- HÃ¤ufigste Kategorien

### **Phase 3: DatenqualitÃ¤t prÃ¼fen**

- Fehlende Werte (NULL, NA) â†’ Wie viele? Wo?
- Duplikate identifizieren
- AusreiÃŸer erkennen
- Inkonsistenzen finden

### **Phase 4: Verteilungen verstehen**

- Histogramme erstellen
- Boxplots fÃ¼r AusreiÃŸer
- Normalverteilung prÃ¼fen (wenn relevant)

### **Phase 5: Beziehungen erkunden**

- Korrelationen zwischen Variablen
- Crosstabs fÃ¼r kategorische Daten
- Scatter Plots fÃ¼r ZusammenhÃ¤nge

## ğŸ¯ Konkrete Tools/Befehle:

**In Python (Pandas):**

python

```python
# Schnell-Ãœbersicht
df.info()              # Struktur, Datentypen
df.describe()          # Deskriptive Statistik
df.head()              # Erste Zeilen
df.isnull().sum()      # Fehlende Werte
df.duplicated().sum()  # Duplikate

# Verteilungen
df['spalte'].value_counts()  # HÃ¤ufigkeiten
df['spalte'].hist()          # Histogramm

# Beziehungen
df.corr()  # Korrelationsmatrix
```

**In SQL:**


```sql
-- Basis-Stats
SELECT 
    COUNT(*) as total_rows,
    COUNT(DISTINCT customer_id) as unique_customers,
    AVG(sales) as avg_sales,
    MIN(sales) as min_sales,
    MAX(sales) as max_sales
FROM sales_data;

-- Fehlende Werte
SELECT 
    SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as missing_emails
FROM customers;
```

**In Excel/Sheets:**

- Pivot Tables
- AVERAGE(), MEDIAN(), STDEV()
- COUNTIF() fÃ¼r HÃ¤ufigkeiten
- Conditional Formatting fÃ¼r AusreiÃŸer

## âš¡ Wann wie viel?

### **Minimum (IMMER):**

- Zeilen/Spalten zÃ¤hlen
- Datentypen prÃ¼fen
- Fehlende Werte checken
- Mean/Median fÃ¼r wichtige Variablen
- Duplikate suchen

### **Standard (normalerweise):**

- VollstÃ¤ndige deskriptive Statistik
- Verteilungen visualisieren
- Korrelationen prÃ¼fen
- AusreiÃŸer identifizieren

### **Intensiv (bei komplexen Projekten):**

- Detaillierte Subgruppen-Analysen
- Multiple Visualisierungen
- Statistische Tests (t-Tests, Chi-Quadrat etc.)
- Bias-Audits

## ğŸš¨ Was passiert OHNE statistische Untersuchung?

**Echte Horror-Beispiele:**

âŒ **Beispiel 1: Fehlende Werte** Ohne Check: "Durchschnittliches Alter: 45 Jahre" Mit Check: "20% fehlende Werte bei Alter - kÃ¶nnten systematisch jÃ¼ngere User sein!"

âŒ **Beispiel 2: AusreiÃŸer** Ohne Check: "Durchschnittlicher Umsatz: 500.000â‚¬" Mit Check: "Ein Mega-Deal von 50 Mioâ‚¬ verzerrt alles - Median ist 5.000â‚¬!"

âŒ **Beispiel 3: Skalierung** Ohne Check: Alter in Jahren (18-80), Einkommen in Euro (20.000-200.000) Problem: ML-Modelle funktionieren schlecht, weil Skalen unterschiedlich!

## ğŸ“‹ Best Practice Workflow:

```
1. Daten importieren
   â†“
2. â­ STATISTISCHE UNTERSUCHUNG (EDA) â­
   â†“
3. Datenbereinigung (basierend auf Findings)
   â†“
4. Nochmal Stats (sind Daten jetzt OK?)
   â†“
5. Eigentliche Analyse
   â†“
6. Ergebnisse interpretieren
```

## ğŸ’¡ Im Kontext deiner Bias-Frage:

Statistische Untersuchungen helfen dir konkret bei Fairness:

**Checkst du z.B.:**

- Geschlechterverteilung: 80% mÃ¤nnlich, 20% weiblich â†’ Bias-Warnung!
- Altersverteilung: Keine Personen Ã¼ber 60 â†’ Bias-Warnung!
- Geografische Verteilung: 95% aus GroÃŸstÃ¤dten â†’ Bias-Warnung!

## âœ… Zusammenfassung:

**Statistische Datenuntersuchungen sind:**

1. âœ“ Obligatorisch, nicht optional
2. âœ“ Der erste Schritt nach Daten-Import
3. âœ“ Essentiell fÃ¼r Bias-Erkennung
4. âœ“ Basis fÃ¼r alle weiteren Entscheidungen
5. âœ“ Teil jeder Data Analytics Best Practice

**Faustregel:** Mindestens 20-30% deiner Analyse-Zeit sollte in EDA flieÃŸen!

---

