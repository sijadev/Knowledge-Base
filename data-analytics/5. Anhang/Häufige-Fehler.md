---
autor: Simon Janke
title: HÃ¤ufige Datenanalyse-Fehler
type: Ressource
date: 2025-10-12
tags:
  - fehler
  - pitfalls
  - antipatterns
  - lessons-learned
---

# âš ï¸ HÃ¤ufige Datenanalyse-Fehler & Wie man sie vermeidet

Ein umfassender Guide zu den klassischen Fallstricken in der Datenanalyse.

---

## ğŸ”´ Kategorie 1: Statistische Fehler

### Fehler 1.1: Korrelation als KausalitÃ¤t interpretieren

**âŒ Der Fehler:**
```
Beobachtung: EisverkÃ¤ufe und ErtrinkungsunfÃ¤lle korrelieren positiv

Falsche Schlussfolgerung:
"EisverkÃ¤ufe verursachen Ertrinken â†’ Eisverkauf verbieten!"

Richtig:
Beide werden durch einen dritten Faktor (Sommer/Hitze) verursacht
```

**Reales Business-Beispiel:**
```
Daten zeigen: 
- Kunden mit Premium-Support haben 50% hÃ¶here Retention
- Manch manager: "Geben wir allen Premium-Support!"

ABER: Causation â‰  Correlation
- Premium-Kunden sind bereits high-value und engaged
- Support ist Korrelat, nicht Ursache der Retention
- Experiment nÃ¶tig um KausalitÃ¤t zu beweisen
```

**âœ… So vermeiden:**
1. Immer fragen: "KÃ¶nnte X auch Y verursachen?" (reverse causation)
2. "Gibt es einen dritten Faktor Z?"
3. Nur A/B-Tests oder kontrollierte Experimente beweisen KausalitÃ¤t
4. Bei starker Korrelation: Hypothese bilden, dann testen

---

### Fehler 1.2: P-Hacking / Cherry Picking

**âŒ Der Fehler:**
```
Analyst testet 20 verschiedene Hypothesen und berichtet nur die eine,
die p < 0.05 zeigt, ohne zu erwÃ¤hnen, dass 19 andere tests durchgefÃ¼hrt wurden.

Bei 20 Tests mit Î±=0.05 erwarte ich 1 "false positive" by chance!
```

**Beispiel:**
```
Test 1: Mobile vs Desktop Conversion â†’ p = 0.32 âŒ
Test 2: iOS vs Android Conversion â†’ p = 0.18 âŒ
Test 3: Chrome vs Safari Conversion â†’ p = 0.71 âŒ
...
Test 15: Dienstag vs Donnerstag Conversion â†’ p = 0.03 âœ“

Report: "Dienstag hat signifikant hÃ¶here Conversion!"
(ohne die 14 failed tests zu erwÃ¤hnen)
```

**âœ… So vermeiden:**
1. **Pre-register** Hypothesen vor Analyse
2. **Bonferroni-Korrektur** bei multiple testing (p-Wert / Anzahl Tests)
3. Alle Tests dokumentieren, nicht nur signifikante
4. Replikation in neuen Daten

---

### Fehler 1.3: Simpson's Paradox ignorieren

**âŒ Der Fehler:**
Ein Trend in aggregierten Daten kehrt sich um, wenn man nach Gruppen segmentiert.

**Klassisches Beispiel - Uni-Admission:**
```
Gesamt:
- 40% MÃ¤nner zugelassen
- 30% Frauen zugelassen
â†’ "Bias gegen Frauen!"

Pro Department:
Dept A: 60% MÃ¤nner, 70% Frauen zugelassen
Dept B: 20% MÃ¤nner, 30% Frauen zugelassen

Problem: Frauen bewerben sich mehr bei Dept B (schwerer)
```

**Business-Beispiel:**
```
Gesamt: Kampagne A hat hÃ¶here Conversion als B

Aber segmentiert:
- Mobile: B besser als A
- Desktop: B besser als A

ErklÃ¤rung: A bekommt mehr Desktop-Traffic (konvertiert generell besser)
```

**âœ… So vermeiden:**
1. **Immer segmentieren** nach wichtigen Dimensionen
2. Confounding Variables identifizieren
3. Multivariate Analysis statt univariate

---

### Fehler 1.4: Zu kleine Stichproben

**âŒ Der Fehler:**
```
A/B-Test mit 50 Nutzern pro Variante
â†’ 40% vs 30% Conversion
â†’ "Variante A gewinnt!"

ABER: Mit so wenig Daten ist das pure Zufallsschwankung
```

**Minimum Sample Sizes:**
```
FÃ¼r aussagekrÃ¤ftige A/B-Tests:
- Conversion ~5%: >2000 pro Variante
- Conversion ~10%: >1000 pro Variante  
- Conversion ~25%: >400 pro Variante

Minimum fÃ¼r jede Analyse: >30 (Central Limit Theorem)
Besser: >100
Ideal: >1000
```

**âœ… So vermeiden:**
1. **Sample Size Calculator** vor Test nutzen
2. Power Analysis durchfÃ¼hren
3. Mindestlaufzeit einhalten (1-2 Wochen fÃ¼r A/B-Tests)
4. Bei kleinen Samples: Vorsicht mit Schlussfolgerungen

---

### Fehler 1.5: Survivorship Bias

**âŒ Der Fehler:**
Nur "Ãœberlebende" analysieren und daraus SchlÃ¼sse ziehen.

**Klassisches Beispiel - WW2 Flugzeuge:**
```
MilitÃ¤r analysiert zurÃ¼ckgekehrte Flugzeuge
â†’ BeschÃ¤digungen an FlÃ¼geln und Rumpf
â†’ Vorschlag: Diese Bereiche verstÃ¤rken

FEHLER: Die Flugzeuge mit Motor-SchÃ¤den sind NICHT zurÃ¼ckgekehrt!
â†’ Motoren mÃ¼ssen verstÃ¤rkt werden
```

**Business-Beispiele:**
```
1. "Erfolgreiche Startups haben alle frÃ¼h gepivoted"
   â†’ Falsch: Du siehst nicht die, die gepivoted UND gescheitert sind
   
2. "Unsere besten Kunden nutzen alle Feature X"
   â†’ Was ist mit Churn? Haben die auch Feature X genutzt?
   
3. "Nutzer die > 10 Sessions haben, werden loyal"
   â†’ Reverse: Loyale Nutzer haben > 10 Sessions (nicht KausalitÃ¤t)
```

**âœ… So vermeiden:**
1. **Kontrolle fÃ¼r "Nicht-Ãœberlebende"** (Churn, Failed Projects, etc.)
2. **Cohort-Analysen** (alle Nutzer tracken, nicht nur aktive)
3. Frage: "Wen sehe ich NICHT in den Daten?"

---

## ğŸŸ¡ Kategorie 2: DatenqualitÃ¤ts-Fehler

### Fehler 2.1: Garbage In, Garbage Out

**âŒ Der Fehler:**
Analyse auf schmutzigen Daten ohne Validierung.

**Beispiele:**
```
1. Tracking-Fehler:
   - Event fired 3x statt 1x
   - â†’ Revenue 3x Ã¼berschÃ¤tzt
   
2. Duplikate:
   - Gleicher User mit 5 IDs
   - â†’ Unique Users 5x Ã¼berschÃ¤tzt
   
3. Fehlende Werte als 0:
   - NULL Revenue â†’ 0
   - â†’ Durchschnitt falsch berechnet
   
4. Typ-Fehler:
   - "123" (string) + "456" = "123456" statt 579
```

**âœ… So vermeiden:**
1. **Data Quality Checks** IMMER first step:
   ```sql
   -- Duplikate check
   SELECT id, COUNT(*) 
   FROM table GROUP BY id HAVING COUNT(*) > 1;
   
   -- NULL check
   SELECT COUNT(*) AS nulls 
   FROM table WHERE important_field IS NULL;
   
   -- Outliers check
   SELECT MIN(value), MAX(value), AVG(value), STDDEV(value)
   FROM table;
   ```

2. **Profiling** vor Analyse
3. **Stichproben manuell prÃ¼fen**
4. **Validierung gegen bekannte Benchmarks**

---

### Fehler 2.2: Sampling Bias

**âŒ Der Fehler:**
Stichprobe ist nicht reprÃ¤sentativ fÃ¼r Population.

**Beispiele:**
```
1. Online-Survey Bias:
   - Nur tech-savvy Nutzer antworten
   - â†’ ÃœberschÃ¤tzt digitale AffinitÃ¤t
   
2. Voluntary Response Bias:
   - Nur sehr zufriedene/unzufriedene antworten
   - â†’ Extreme Meinungen Ã¼berreprÃ¤sentiert
   
3. Time-of-Day Bias:
   - Analyse nur Workday-Daten
   - â†’ Ãœbersieht Weekend-Nutzer
   
4. Geographic Bias:
   - Test nur in US
   - â†’ Nicht Ã¼bertragbar auf Europa/Asia
```

**âœ… So vermeiden:**
1. **Random Sampling** wo mÃ¶glich
2. **Stratified Sampling** fÃ¼r wichtige Segmente
3. **Quotas** fÃ¼r Ausgewogenheit
4. **Weights** fÃ¼r ReprÃ¤sentativitÃ¤t
5. Dokumentiere Sampling-Methode

---

### Fehler 2.3: Daten-Leakage

**âŒ Der Fehler:**
Future Information in Training-Daten (bei ML-Modellen).

**Beispiel:**
```
Problem: Churn Prediction
Features: 
- Customer_ID
- Last_Login_Date  â† LEAKAGE!
- Support_Tickets
- Churned (Target)

Warum Leakage?
- Gechurnete Kunden haben natÃ¼rlich kein recent Login
- Modell lernt: "Kein Login = Churn"
- ABER: In Produktion weiÃŸ ich nicht wer einloggen wird!
```

**âœ… So vermeiden:**
1. **Temporal Validation**: Train auf Vergangenheit, test auf Zukunft
2. Nur Features nutzen, die **vor** Target bekannt sind
3. "KÃ¶nnte ich diese Info zum Predictions-Zeitpunkt haben?"

---

## ğŸŸ  Kategorie 3: Analyse-Design-Fehler

### Fehler 3.1: HARKing (Hypothesizing After Results are Known)

**âŒ Der Fehler:**
Nach dem Sehen der Daten eine "Hypothese" formulieren, als wÃ¤re sie vor der Analyse erstellt worden.

**Beispiel:**
```
Analyst schaut Daten an:
"Oh, Mobile Conversion ist 10% niedriger!"

Report:
"Wir HYPOTHESIERTEN dass Mobile Conversion niedriger sei..."

Problem: Das ist keine echte Hypothese, sondern post-hoc ErklÃ¤rung
```

**âœ… So vermeiden:**
1. **Pre-register** Hypothesen
2. Exploratory vs. Confirmatory Analysis trennen
3. Transparent Ã¼ber Discovery vs. Validation

---

### Fehler 3.2: One-Size-Fits-All Metrics

**âŒ Der Fehler:**
Gleiche Metrik fÃ¼r verschiedene Kontexte nutzen.

**Beispiele:**
```
1. "Average Revenue per User" (ARPU)
   Problem: 
   - Bei skewed distributions irrefÃ¼hrend
   - Ein Whale-Kunde kann Durchschnitt verzerren
   Besser: Median + P90

2. "Average Session Duration"
   Problem:
   - LÃ¤nger â‰  Besser
   - Support-Seiten: Lang = Problem findet keine LÃ¶sung
   - Content-Seiten: Lang = Engagement
   Besser: Kontext-spezifische Metriken

3. "Click-Through-Rate" (CTR)
   Problem:
   - Hohe CTR, aber keine Conversion
   - Clickbait performiert "gut"
   Besser: Multi-metric evaluation
```

**âœ… So vermeiden:**
1. **North Star Metric** definieren (kontextabhÃ¤ngig)
2. **Counter-Metrics** fÃ¼r jede Primary-Metric
3. **Guardrail Metrics** fÃ¼r QualitÃ¤t
4. Segmentierte Metriken

---

### Fehler 3.3: Comparing Apples to Oranges

**âŒ Der Fehler:**
Unfairer Vergleich durch unterschiedliche Bedingungen.

**Beispiele:**
```
1. Zeitvergleich ohne Seasonality:
   "Dezember-Sales sind 200% hÃ¶her als November!"
   â†’ NatÃ¼rlich wegen Weihnachten
   
2. Geo-Vergleich ohne Normalisierung:
   "USA hat 10x mehr Sales als Schweiz!"
   â†’ Ja, aber 40x mehr Einwohner
   
3. Produkt-Vergleich mit unterschiedlichem Age:
   "Product A hat mehr Users als B"
   â†’ A ist seit 5 Jahren live, B seit 2 Monaten
```

**âœ… So vermeiden:**
1. **Like-for-like** Vergleiche
2. **Normalisierung** (per Capita, per Day, etc.)
3. **Cohort-Vergleiche** statt absolute
4. **Control fÃ¼r Confounders**

---

## ğŸ”µ Kategorie 4: Kommunikations-Fehler

### Fehler 4.1: Fehlender Kontext

**âŒ Der Fehler:**
Zahlen ohne Vergleichswerte prÃ¤sentieren.

**Schlecht:**
```
"Wir haben 10.000 neue User!"
```

**Besser:**
```
"Wir haben 10.000 neue User:
- vs 8.000 letzten Monat (+25%)
- vs Target von 12.000 (-17%)
- vs Industry Growth von +15% (outperforming)
- Haupttreiber: Organic Search (+40%)
```

**âœ… So vermeiden:**
Immer 3 Kontexte geben:
1. **Temporal**: vs Vorperiode
2. **Target**: vs Ziel/Benchmark
3. **Why**: Treiber der VerÃ¤nderung

---

### Fehler 4.2: Ãœberkomplizierte Visualisierungen

**âŒ Der Fehler:**
```
- 15 Lines in einem Chart
- 3D Pie Charts
- Dual Axes mit unterschiedlichen Skalen
- Zu viele Farben
```

**âœ… So vermeiden:**
1. **One Chart, One Message**
2. **Max 3-5 Series** in einem Chart
3. **Simple > Complex**
4. **Test**: "Kann jemand in 5 Sekunden verstehen?"

---

### Fehler 4.3: False Precision

**âŒ Der Fehler:**
Zu viele Dezimalstellen suggerieren hÃ¶here Genauigkeit als existiert.

**Schlecht:**
```
"Conversion Rate ist 2.847291%"
```

**Besser:**
```
"Conversion Rate ist ~2.8%"
oder
"Conversion Rate ist 2.8% Â± 0.3% (95% CI)"
```

**âœ… So vermeiden:**
1. Runde auf **significant figures**
2. **Konfidenzintervalle** statt PunktschÃ¤tzungen
3. "Approximately" / "About" / "~" nutzen

---

## ğŸŸ¢ Kategorie 5: Business-Logic-Fehler

### Fehler 5.1: Optimierung der falschen Metrik

**âŒ Der Fehler:**
Lokales Optimum statt globales.

**Beispiele:**
```
1. E-Commerce optimiert "Add to Cart" Rate
   â†’ Nutzer adden, aber kaufen nicht
   â†’ Revenue sinkt trotz hÃ¶herer Cart-Rate
   
2. Content-Site optimiert "Time on Page"
   â†’ Nutzer finden Info nicht â†’ lesen lÃ¤nger
   â†’ Schlechte UX, aber "gute" Metrik
   
3. Support optimiert "Ticket Close Rate"
   â†’ Tickets schnell geschlossen ohne LÃ¶sung
   â†’ Customer Satisfaction sinkt
```

**âœ… So vermeiden:**
1. **End-to-End** Metriken priorisieren (Revenue, CSAT, Retention)
2. **Counter-Metrics** immer monitoren
3. **A/B Test** ganze Customer Journey
4. Qualitative Feedback zusÃ¤tzlich zu Quantitative

---

### Fehler 5.2: Short-term vs Long-term Trade-off ignorieren

**âŒ Der Fehler:**
Nur kurzfristige Gewinne optimieren, langfristige SchÃ¤den ignorieren.

**Beispiele:**
```
1. Aggressive Email-Kampagnen:
   Short-term: +20% Revenue
   Long-term: +50% Unsubscribes, Brand Damage
   
2. Discount-Strategie:
   Short-term: Mehr Sales
   Long-term: Kunden warten auf Discounts, Margin erodiert
   
3. Clickbait-Headlines:
   Short-term: Higher CTR
   Long-term: Lower Trust, Bounce Rate steigt
```

**âœ… So vermeiden:**
1. **Leading vs Lagging** Indicators tracken
2. **Cohort-basierte** Long-term Metrics
3. **LTV-Modelle** statt nur Acquisition
4. **Brand Health** Metrics parallel

---

### Fehler 5.3: Ignoring Implementation Costs

**âŒ Der Fehler:**
Empfehlungen ohne Kosten-Nutzen-Analyse.

**Beispiel:**
```
Analyst: "Wenn wir Ladezeit um 1s reduzieren, steigt Conversion um 2%!"

CEO: "Great! Wie?"

Analyst: "Ã„hm... komplettes Rewrite der Infrastruktur?"

â†’ 2% Conversion-Lift = $100k Revenue
â†’ Rewrite kostet $1M + 6 Monate
â†’ ROI negativ!
```

**âœ… So vermeiden:**
1. **Impact vs Effort** Matrix
2. **Quick Wins** identifizieren
3. **Engineering-Input** vor Empfehlungen
4. **Phased Rollout** Optionen

---

## ğŸ› ï¸ Best Practices: Fehler-PrÃ¤vention

### 1. Pre-Analysis Checklist
```
â–¡ Hypothese klar formuliert
â–¡ Sample Size berechnet
â–¡ Confounders identifiziert
â–¡ Data Quality geprÃ¼ft
â–¡ Bias-Quellen berÃ¼cksichtigt
â–¡ Success Criteria definiert
```

### 2. During Analysis
```
â–¡ Dokumentiere jeden Schritt
â–¡ Stichproben manuell prÃ¼fen
â–¡ Segmentierte Analysen
â–¡ Sanity Checks (stimmen Totals?)
â–¡ Peer Review Code/Queries
```

### 3. Post-Analysis
```
â–¡ Results validieren (separate Dataset)
â–¡ Alternative ErklÃ¤rungen prÃ¼fen
â–¡ Limitations dokumentieren
â–¡ Confidence Levels angeben
â–¡ Stakeholder-Feedback einholen
```

### 4. Communication
```
â–¡ Kontext immer geben
â–¡ Unsicherheit transparent machen
â–¡ Visualisierungen simpel halten
â–¡ Actionable Recommendations
â–¡ Follow-up Plan definieren
```

---

## ğŸ“š Lernressourcen

**BÃ¼cher:**
- "Naked Statistics" - Charles Wheelan
- "How to Lie with Statistics" - Darrell Huff
- "The Signal and the Noise" - Nate Silver

**Papers:**
- "Statistical Mistakes and How to Avoid Them" - Various

**Online:**
- [Calling Bullshit](https://callingbullshit.org/) - Course on Data Reasoning
- [Spurious Correlations](https://tylervigen.com/spurious-correlations) - Fun Examples

---

## âœ… Quick Reference Card

**Top 10 Fehler vermeiden:**
1. âœ… Korrelation â‰  KausalitÃ¤t
2. âœ… Sample Size matters
3. âœ… Segmentiere immer
4. âœ… Check Data Quality first
5. âœ… Beware Survivorship Bias
6. âœ… Pre-register Hypothesen
7. âœ… Give Context
8. âœ… Monitor Counter-Metrics
9. âœ… Consider Long-term
10. âœ… Document Assumptions

**Frage bei jeder Analyse:**
- "KÃ¶nnte das Zufall sein?"
- "Wen/Was sehe ich NICHT in den Daten?"
- "Gibt es alternative ErklÃ¤rungen?"
- "Welche Annahmen mache ich?"
- "Wie robust ist meine Schlussfolgerung?"

---

*Aktualisiert: Oktober 2025*